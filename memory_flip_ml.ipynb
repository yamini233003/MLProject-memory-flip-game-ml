
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt

# Step 1: Create dummy data
np.random.seed(42)
data = {
    'grid_size': np.random.choice([4, 6, 8], 200),
    'num_symbols': np.random.choice([8, 18, 32], 200),
    'allowed_mistakes': np.random.choice([5, 10, 15], 200),
    'time_to_solve': np.random.randint(30, 300, 200)
}
df = pd.DataFrame(data)

# Step 2: Create difficulty labels
def label_difficulty(row):
    if row['time_to_solve'] < 60:
        return 'Easy'
    elif row['time_to_solve'] < 150:
        return 'Medium'
    else:
        return 'Hard'

df['difficulty'] = df.apply(label_difficulty, axis=1)

# Step 3: Train-test split
X = df[['grid_size', 'num_symbols', 'allowed_mistakes', 'time_to_solve']]
y = df['difficulty']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 4: Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Step 5: Evaluate model
predictions = model.predict(X_test)
print(confusion_matrix(y_test, predictions))
print(classification_report(y_test, predictions))

# Step 6: Feature importance
importances = model.feature_importances_
features = X.columns
indices = np.argsort(importances)

plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='b', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()
